name: Scrape new data
on:
  repository_dispatch:
    types: scrape
  # TODO Ideally this will only run when changes to the scraper folder are made
  push:
    branches:
      - master
  schedule:
    - cron: '0 * * * *'
  workflow_dispatch:
    inputs:
      scrape_all:
        description: 'Scrape all semesters (ALL_YEARS|CURRENT_YEAR)'
        required: true
        default: 'CURRENT_YEAR'

jobs:
  scrape-courses-and-prerequisites:
    name: Scrapes courses and prerequisites
    runs-on: ubuntu-latest
    steps:
      - name: Checkout branch
        uses: actions/checkout@v2

      - name: Set up python
        uses: actions/setup-python@v2
        with:
          python-version: '3.x'

      - name: Install pip requirements
        run: |
          python -m pip install --upgrade pip
          pip install -r scrapers/requirements.txt

      - name: Clone QuACS Data
        uses: actions/checkout@v2
        with:
          repository: quacs/quacs-data
          path: scrapers/quacs-data

      - name: Copy quacs-data files to correct locations
        working-directory: ./scrapers/quacs-data
        run: |
          cp all_schools.json ../all_schools.json
          cp semester_data ../data -r

      - name: Scrape courses
        working-directory: ./scrapers
        run: python3 sis_scraper/main.py ${{ github.event.inputs.scrape_all }}

      - name: Upload semester-specific data
        uses: actions/upload-artifact@v2
        with:
          name: courses
          path: scrapers/data/

  scrape-prereq-graph:
    name: Scrapes prerequisite graph
    runs-on: ubuntu-latest
    needs: [scrape-courses-and-prerequisites]
    steps:
      - name: Checkout branch
        uses: actions/checkout@v2

      - name: Set up python
        uses: actions/setup-python@v2
        with:
          python-version: "3.x"

      - name: Install pip requirements
        run: |
          python -m pip install --upgrade pip
          pip install -r scrapers/requirements.txt

      # The prereq graph scraper turns data from all semesters into a single json,
      # so we need all semester data in a single directory.
      # The output of the SIS scraper is only current semesters,
      # so we need to merge the SIS scraper output
      # with the data already in the quacs-data repo.

      - name: Checkout data repository
        uses: actions/checkout@v2
        with:
          path: "scrapers/data"
          repository: "quacs/quacs-data"
          ref: "master"
          clean: true
          token: ${{ secrets.GITHUBTOKEN }}

      - name: Get scraped data
        uses: actions/download-artifact@v2
        with:
          path: scrapers

      - name: Copy newly-scraped semester data to directory
        working-directory: ./scrapers
        run: rsync -avz courses/ data/semester_data/

      - name: Build prerequisites graph
        working-directory: ./scrapers
        run: python3 prerequisites_graph/main.py data/semester_data prereq_graph.json

      - name: Upload prerequisite graph data
        uses: actions/upload-artifact@v2
        with:
          name: prereq_graph
          path: scrapers/prereq_graph.json

  # This may need to be updated now that the scrapers have been moved into the same repo as the site
  # scrape-degree-requirements:
  #   name: Scrape degree requirements
  #   runs-on: ubuntu-latest
  #   steps:
  #     - name: Checkout degree planner
  #       uses: actions/checkout@v2
  #       with:
  #         path: 'degree-planner'
  #         repository: 'quacs/degree-planner'

  #     - name: Install geckodriver
  #       run: |
  #           export GECKODRIVER_VERSION="v0.27.0"
  #           wget https://github.com/mozilla/geckodriver/releases/download/v0.27.0/geckodriver-$GECKODRIVER_VERSION-linux64.tar.gz
  #           tar -xvzf geckodriver-$GECKODRIVER_VERSION-linux64.tar.gz
  #           chmod +x geckodriver

  #     - name: Set up Python
  #       uses: actions/setup-python@v2
  #       with:
  #         python-version: '3.x'

  #     - name: Set up Rust
  #       uses: actions-rs/toolchain@v1
  #       with:
  #         toolchain: stable

  #     - name: Install pip requirements
  #       run: |
  #         python -m pip install --upgrade pip
  #         pip install -r degree-planner/requirements.txt

  #     - name: Populate env
  #       run: printf "RIN=${{ secrets.RIN }}\nPASSWORD=${{ secrets.PASSWORD }}" > degree-planner/.env

  #     - name: Scrape courses
  #       run: |
  #           cd degree-planner
  #           python3 scraper.py refresh_data

  #     - name: Upload data
  #       uses: actions/upload-artifact@v2
  #       with:
  #         name: degree_requirements
  #         path: degree-planner/*.json

  scrape-hass-pathways:
    name: Scrape HASS pathways
    runs-on: ubuntu-latest
    timeout-minutes: 1
    steps:
      - name: Checkout branch
        uses: actions/checkout@v2

      - name: Set up python
        uses: actions/setup-python@v2
        with:
          python-version: '3.x'

      - name: Install pip requirements
        run: |
          python -m pip install --upgrade pip
          pip install -r scrapers/requirements.txt

      - name: Scrape HASS pathways
        working-directory: ./scrapers
        run: python3 hass_pathways_scraper/main.py > hass_pathways.json

      - name: Upload data
        uses: actions/upload-artifact@v2
        with:
          name: hass_pathways
          path: scrapers/hass_pathways.json

  scrape-faculty:
    name: Scrape faculty
    runs-on: ubuntu-latest
    steps:
      - name: Checkout branch
        uses: actions/checkout@v2

      - name: Set up python
        uses: actions/setup-python@v2
        with:
          python-version: '3.x'

      - name: Install pip requirements
        run: |
          python -m pip install --upgrade pip
          pip install -r scrapers/requirements.txt

      - name: Scrape faculty
        working-directory: ./scrapers
        run: python3 faculty_directory_scraper/main.py

      - name: Upload data
        uses: actions/upload-artifact@v2
        with:
          name: faculty
          path: scrapers/faculty.json

  scrape-transfer:
    name: Scrape transfer
    runs-on: ubuntu-latest
    steps:
      - name: Checkout branch
        uses: actions/checkout@v2

      - name: Set up python
        uses: actions/setup-python@v2
        with:
          python-version: '3.x'

      - name: Install pip requirements
        run: |
          python -m pip install --upgrade pip
          pip install -r scrapers/requirements.txt

      - name: Scrape transfer
        working-directory: ./scrapers
        run: python3 transfer_scraper/main.py csv

      - name: Upload JSON data
        uses: actions/upload-artifact@v2
        with:
          name: transfer
          path: scrapers/transfer.json

      - name: Upload CSV data
        uses: actions/upload-artifact@v2
        with:
          name: transfer_guides
          path: scrapers/transfer_guides

  # This may need to be updated now that the scrapers have been moved into the same repo as the site
  scrape-catalog:
    name: Scrapes catalog by year
    runs-on: ubuntu-latest
    steps:
      - name: Checkout branch
        uses: actions/checkout@v2

      - name: Set up python
        uses: actions/setup-python@v2
        with:
          python-version: '3.x'

      - name: Install pip requirements
        run: |
          python -m pip install --upgrade pip
          pip install -r scrapers/requirements.txt

      - name: Scrape catalog
        working-directory: ./scrapers
        run: python3 catalog_scraper/main.py ${{ github.event.inputs.scrape_all }}

      - name: Upload data
        uses: actions/upload-artifact@v2
        with:
          name: catalog
          path: scrapers/data

  commit-data:
    name: Commit changes
    runs-on: ubuntu-latest
    needs:
      - scrape-courses-and-prerequisites
      - scrape-faculty
      # - scrape-hass-pathways
      - scrape-prereq-graph
      - scrape-catalog
    steps:
      - name: Checkout data repository
        uses: actions/checkout@v2
        with:
          path: "scrapers/data"
          repository: "quacs/quacs-data"
          ref: "master"
          clean: true
          token: ${{ secrets.GITHUBTOKEN }}

      - name: Get scraped data
        uses: actions/download-artifact@v2
        with:
          path: scrapers

      - name: Commit new data
        working-directory: ./scrapers
        run: |
          for directory in $(find courses/* -type d -maxdepth 0 -print0 | xargs -0); do
            DIR_BASENAME=$(basename "$directory")
            cp -r catalog/$DIR_BASENAME/* "$directory" | true
          done

          rsync -avz courses/ data/semester_data/

          cp prereq_graph/prereq_graph.json data/

          rsync -avzh --ignore-missing-args faculty/faculty.json data || true

          # rsync -avzh --ignore-missing-args hass_pathways/hass_pathways.json data || true

          rsync -avzh --ignore-missing-args transfer/transfer.json data || true

          rsync -avzh --ignore-missing-args transfer_guides/ data/transfer_guides/ || true

          cd data

          # Merge any hard_coded data into the final values
          for directory in $(find semester_data/* -type d  | grep -v "hard_coded" | xargs -0); do
            if [ -d "$directory/hard_coded" ]; then
              if [ -f "$directory/hard_coded/catalog.json" ]; then
                  echo "$directory/catalog.json"
                  jq -a -s '.[0] * .[1]' "$directory/catalog.json" "$directory/hard_coded/catalog.json" > "$directory/temp_catalog.json"
                  mv "$directory/temp_catalog.json" "$directory/catalog.json"
                  truncate -s -1 "$directory/catalog.json" #jq adds a newline at the end of the file that we dont want
              fi
            fi
          done

          # Generate catalogs for any folders missing catalogs
          # This is to minimize build failures when a semester transition occurs.
          for directory in $(find semester_data/* -type d | xargs -0); do
            if [ ! -f "$directory/catalog.json" ]; then
              echo "Generating $directory/catalog.json"
              echo "{}" > $directory/catalog.json
            fi
          done

          git config user.name "QuACS" && git config user.email "github@quacs.org"
          echo {\"last_updated\":\"$(date --iso-8601=seconds -u)\"} > meta.json
          git add .
          git commit -m "$(date -u)" # || exit 0
          #git add meta.json
          #git commit --amend --no-edit

          git push --force
          curl -H "Accept: application/vnd.github.everest-preview+json" \
              -H "Authorization: token ${{ secrets.GITHUBTOKEN }}" \
              --request POST \
              --data '{"event_type": "deploy", "client_payload": {"build_args": "-d"}}' \
              https://api.github.com/repos/quacs/quacs/dispatches


  # datadog-metrics:
  #   name: Send Datadog metrics
  #   runs-on: ubuntu-latest
  #   needs:
  #     - commit-data
  #   steps:
  #     - name: Install Datadog agent
  #       run: |
  #         DD_AGENT_MAJOR_VERSION=7 DD_API_KEY=${{ secrets.DATADOG_API_KEY }} DD_SITE="datadoghq.com" bash -c "$(curl -L https://s3.amazonaws.com/dd-agent/scripts/install_script.sh)"

  #     - name: Checkout branch
  #       uses: actions/checkout@v2

  #     - name: Set up python
  #       uses: actions/setup-python@v2
  #       with:
  #         python-version: "3.x"

  #     - name: Install pip requirements
  #       run: |
  #         python -m pip install --upgrade pip
  #         pip install -r scrapers/requirements.txt

  #     - name: Checkout data repository
  #       uses: actions/checkout@v2
  #       with:
  #         path: "scrapers/data"
  #         repository: "quacs/quacs-data"
  #         ref: "master"
  #         clean: true
  #         token: ${{ secrets.GITHUBTOKEN }}

  #     - name: Generate and send metrics
  #       working-directory: ./scrapers
  #       run: python3 datadog/send_metrics.py
