name: Scrape new data
on:
  repository_dispatch:
    types: scrape
  # TODO Ideally this will only run when changes to the scraper folder are made
  # push:
  #   branches:
  #     - master
  schedule:
    - cron: '0 * * * *'

jobs:
  scrape-schools:
    name: Scrapes schools per year
    runs-on: ubuntu-latest
    steps:
      - name: Cancel Previous Runs
        uses: styfle/cancel-workflow-action@0.6.0
        with:
          access_token: ${{ github.token }}

      - name: Print event name
        run: echo $GITHUB_EVENT_NAME

      - name: Checkout branch
        uses: actions/checkout@v2

      - name: Set up python
        uses: actions/setup-python@v2
        with:
          python-version: '3.x'

      - name: Install pip requirements
        run: |
          python -m pip install --upgrade pip
          pip install -r scrapers/requirements.txt

      - name: Scrape schools
        working-directory: ./scrapers
        run: python3 catalog_scraper/main.py schools LATEST_YEAR

      - name: Output semesters
        working-directory: ./scrapers
        run: |
          echo -n "::set-output name=semesters::["
          ITER=0
          for directory in $(find data -print0 | xargs -0); do
            if test $ITER -ne 0; then
              echo -n ","
            fi
            echo -n "\"$directory\""
            ITER=$((ITER + 1))
          done
          echo "]"

      - name: Upload data
        uses: actions/upload-artifact@v2
        with:
          name: schools
          path: scrapers/data/

  scrape-courses-and-prerequisites:
    name: Scrapes courses and prerequisites
    runs-on: ubuntu-latest
    needs: [scrape-schools]
    steps:
      - name: Checkout branch
        uses: actions/checkout@v2

      - name: Set up python
        uses: actions/setup-python@v2
        with:
          python-version: '3.x'

      - name: Install pip requirements
        run: |
          python -m pip install --upgrade pip
          pip install -r scrapers/requirements.txt

      - name: Populate env
        working-directory: ./scrapers
        run: printf "RIN=${{ secrets.RIN }}\nPASSWORD=${{ secrets.PASSWORD }}" > sis_scraper/.env

      - name: Get semester data
        uses: actions/download-artifact@v2
        with:
          name: schools
          path: scrapers/data

      - name: Scrape courses
        working-directory: ./scrapers
        run: python3 sis_scraper/main.py

      - name: Scrape prerequisites
        working-directory: ./scrapers
        run: python3 prerequisites_scraper/main.py

      - name: Upload semester-specific data
        uses: actions/upload-artifact@v2
        with:
          name: courses
          path: scrapers/data/

  scrape-prereq-graph:
    name: Scrapes prerequisite graph
    runs-on: ubuntu-latest
    needs: [scrape-courses-and-prerequisites]
    steps:
      - name: Checkout branch
        uses: actions/checkout@v2

      - name: Set up python
        uses: actions/setup-python@v2
        with:
          python-version: "3.x"

      - name: Install pip requirements
        run: |
          python -m pip install --upgrade pip
          pip install -r scrapers/requirements.txt

      # The prereq graph scraper turns data from all semesters into a single json,
      # so we need all semester data in a single directory.
      # The output of the SIS scraper is only current semesters,
      # so we need to merge the SIS scraper output
      # with the data already in the quacs-data repo.

      - name: Checkout data repository
        uses: actions/checkout@v2
        with:
          path: "scrapers/data"
          repository: "quacs/quacs-data"
          ref: "master"
          clean: true
          token: ${{ secrets.GITHUBTOKEN }}

      - name: Get scraped data
        uses: actions/download-artifact@v2
        with:
          path: scrapers

      - name: Copy newly-scraped semester data to directory
        working-directory: ./scrapers
        run: rsync -avz courses/ data/semester_data/

      - name: Build prerequisites graph
        working-directory: ./scrapers
        run: python3 prerequisites_graph/main.py data/semester_data prereq_graph.json

      - name: Upload prerequisite graph data
        uses: actions/upload-artifact@v2
        with:
          name: prereq_graph
          path: scrapers/prereq_graph.json

  # This may need to be updated now that the scrapers have been moved into the same repo as the site
  # scrape-degree-requirements:
  #   name: Scrape degree requirements
  #   runs-on: ubuntu-latest
  #   steps:
  #     - name: Checkout degree planner
  #       uses: actions/checkout@v2
  #       with:
  #         path: 'degree-planner'
  #         repository: 'quacs/degree-planner'

  #     - name: Install geckodriver
  #       run: |
  #           export GECKODRIVER_VERSION="v0.27.0"
  #           wget https://github.com/mozilla/geckodriver/releases/download/v0.27.0/geckodriver-$GECKODRIVER_VERSION-linux64.tar.gz
  #           tar -xvzf geckodriver-$GECKODRIVER_VERSION-linux64.tar.gz
  #           chmod +x geckodriver

  #     - name: Set up Python
  #       uses: actions/setup-python@v2
  #       with:
  #         python-version: '3.x'

  #     - name: Set up Rust
  #       uses: actions-rs/toolchain@v1
  #       with:
  #         toolchain: stable

  #     - name: Install pip requirements
  #       run: |
  #         python -m pip install --upgrade pip
  #         pip install -r degree-planner/requirements.txt

  #     - name: Populate env
  #       run: printf "RIN=${{ secrets.RIN }}\nPASSWORD=${{ secrets.PASSWORD }}" > degree-planner/.env

  #     - name: Scrape courses
  #       run: |
  #           cd degree-planner
  #           python3 scraper.py refresh_data

  #     - name: Upload data
  #       uses: actions/upload-artifact@v2
  #       with:
  #         name: degree_requirements
  #         path: degree-planner/*.json

  scrape-hass-pathways:
    name: Scrape HASS pathways
    runs-on: ubuntu-latest
    steps:
      - name: Checkout branch
        uses: actions/checkout@v2

      - name: Set up python
        uses: actions/setup-python@v2
        with:
          python-version: '3.x'

      - name: Install pip requirements
        run: |
          python -m pip install --upgrade pip
          pip install -r scrapers/requirements.txt

      - name: Scrape HASS pathways
        working-directory: ./scrapers
        run: python3 hass_pathways_scraper/main.py > hass_pathways.json

      - name: Upload data
        uses: actions/upload-artifact@v2
        with:
          name: hass_pathways
          path: scrapers/hass_pathways.json

  scrape-faculty:
    name: Scrape faculty
    runs-on: ubuntu-latest
    steps:
      - name: Checkout branch
        uses: actions/checkout@v2

      - name: Set up python
        uses: actions/setup-python@v2
        with:
          python-version: '3.x'

      - name: Install pip requirements
        run: |
          python -m pip install --upgrade pip
          pip install -r scrapers/requirements.txt

      - name: Scrape faculty
        working-directory: ./scrapers
        run: python3 faculty_directory_scraper/main.py

      - name: Upload data
        uses: actions/upload-artifact@v2
        with:
          name: faculty
          path: scrapers/faculty.json

  scrape-transfer:
    name: Scrape transfer
    runs-on: ubuntu-latest
    steps:
      - name: Checkout branch
        uses: actions/checkout@v2

      - name: Set up python
        uses: actions/setup-python@v2
        with:
          python-version: '3.x'

      - name: Install pip requirements
        run: |
          python -m pip install --upgrade pip
          pip install -r scrapers/requirements.txt

      - name: Scrape transfer
        working-directory: ./scrapers
        run: python3 transfer_scraper/main.py csv

      - name: Upload JSON data
        uses: actions/upload-artifact@v2
        with:
          name: transfer
          path: scrapers/transfer.json

      - name: Upload CSV data
        uses: actions/upload-artifact@v2
        with:
          name: transfer_guides
          path: scrapers/transfer_guides

  # This may need to be updated now that the scrapers have been moved into the same repo as the site
  # scrape-catalog:
  #   name: Scrapes catalog by year
  #   runs-on: ubuntu-latest
  #   steps:
  #     - name: Checkout scrapers
  #       uses: actions/checkout@v2
  #       with:
  #         ref: 'master'

  #     - name: Set up python
  #       uses: actions/setup-python@v2
  #       with:
  #         python-version: '3.x'

  #     - name: Install pip requirements
  #       run: |
  #         python -m pip install --upgrade pip
  #         pip install -r requirements.txt

  #     - name: Scrape catalog
  #       run: python3 catalog_scraper/main.py catalog LATEST_YEAR

  #     - name: Upload data
  #       uses: actions/upload-artifact@v2
  #       with:
  #         name: catalog
  #         path: data/

  commit-data:
    name: Commit changes
    runs-on: ubuntu-latest
    needs:
      - scrape-courses-and-prerequisites
      - scrape-faculty
      - scrape-hass-pathways
      - scrape-prereq-graph
    steps:
      - name: Checkout data repository
        uses: actions/checkout@v2
        with:
          path: "scrapers/data"
          repository: "quacs/quacs-data"
          ref: "master"
          clean: true
          token: ${{ secrets.GITHUBTOKEN }}

      - name: Get scraped data
        uses: actions/download-artifact@v2
        with:
          path: scrapers

      - name: Generate meta json file
        working-directory: ./scrapers
        run: echo {\"last_updated\":\"$(date --iso-8601=seconds -u)\"} > data/meta.json

      - name: Commit new data
        working-directory: ./scrapers
        run: |
          # for directory in $(find courses/* -type d -print0 | xargs -0); do
          #   DIR_BASENAME=$(basename "$directory")
          #   cp -r catalog/$DIR_BASENAME/* "$directory"
          # done

          rsync -avz courses/ data/semester_data/

          cp prereq_graph/prereq_graph.json data/

          rsync -avzh --ignore-missing-args faculty/faculty.json data || true

          rsync -avzh --ignore-missing-args hass_pathways/hass_pathways.json data || true

          rsync -avzh --ignore-missing-args transfer/transfer.json data || true

          rsync -avzh --ignore-missing-args transfer_guides/ data/transfer_guides/ || true

          cd data
          git config user.name "QuACS" && git config user.email "github@quacs.org"

          git add .

          git commit -m "$(date -u)"
          git push --force

      - name: Deploy updated website
        working-directory: ./scrapers
        if: ${{ success() }}
        run: |
          cd data
          curl -H "Accept: application/vnd.github.everest-preview+json" \
              -H "Authorization: token ${{ secrets.GITHUBTOKEN }}" \
              --request POST \
              --data '{"event_type": "deploy"}' \
              https://api.github.com/repos/quacs/quacs/dispatches
